{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c713155-f3b8-4bda-8e81-7559b040cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "okitamark\n",
    "HuggingFace AutoModelForCausalLM Streaming\n",
    "\"\"\"\n",
    "\n",
    "## モデルとログファイルをセットする\n",
    "MODEL_NAME = '/home/users/model/ArrowPro-7B-KUJIRA' ## モデルを指定\n",
    "# LOG_FILE = __file__.replace('.py', '.log') ## デフォルトログファイル名, jupyterの時はNone\n",
    "LOG_FILE = None ## ファイル名 or None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Argument, Logging\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "## 引数の処理\n",
    "## jupyterの場合コメント\n",
    "# parser = argparse.ArgumentParser(description='OkitaMark')\n",
    "# parser.add_argument('--model', type=str, default=MODEL_NAME)\n",
    "# parser.add_argument('--log', type=str, default=LOG_FILE)\n",
    "# args = parser.parse_args()\n",
    "# MODEL_NAME = args.model\n",
    "# LOG_FILE = args.log\n",
    "\n",
    "## LOG_FILEが指定されていたら、ログファイルも出力する\n",
    "if LOG_FILE is not None:\n",
    "    handlers=[logging.FileHandler(LOG_FILE, mode='w'), logging.StreamHandler()]\n",
    "else:\n",
    "    handlers=[logging.StreamHandler()]\n",
    "\n",
    "## Logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s', handlers=handlers) ## Message Only\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LOG.info('MODEL_NAME: ' + MODEL_NAME)\n",
    "if LOG_FILE is not None: LOG.info('LOG_FILE:' + LOG_FILE)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Base\n",
    "\"\"\"\n",
    "import os, time\n",
    "import subprocess, argparse\n",
    "import torch, gc\n",
    "## Warning非表示\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "## Logging\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(message)s') ## Message Only\n",
    "# LOG = logging.getLogger(__name__)\n",
    "\n",
    "## Util ##\n",
    "## メモリ関連\n",
    "def show_memory():\n",
    "  bytes = torch.cuda.max_memory_allocated()\n",
    "  return 'GPU allocated memory: {mem:.2f} GiB'.format(mem=(bytes / 1024**3)) # GB\n",
    "\n",
    "def free_memory():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "## GPU Info\n",
    "def gpu_info():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=name --format=csv,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU device: ' + gpuinfo.decode()\n",
    "\n",
    "def gpu_mem():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used --format=csv,,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU memory: ' + gpuinfo.decode()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "モデルの定義\n",
    "Transformer\n",
    "\"\"\"\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "model_name = MODEL_NAME\n",
    "\n",
    "## free GPU Memory\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'tokenizer' in globals():\n",
    "    del tokenizer\n",
    "free_memory()\n",
    "\n",
    "\n",
    "## init Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,  ## モデル名,\n",
    "        device_map=\"auto\", ## auto, cpu, cuda:0\n",
    "        torch_dtype=\"auto\", ## torch.float16,torch.bfloat16\n",
    "        trust_remote_code=True,\n",
    "        # load_in_8bit=True, ## 8bit　bitsandbytes\n",
    "    )\n",
    "\n",
    "\n",
    "## init Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,  ## モデル名\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "\n",
    "# ストリーミング表示\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True, ## 入力文は返さない\n",
    "    skip_special_tokens=True ## </s> などの特殊トークンも不要\n",
    ")\n",
    "\n",
    "\n",
    "## 推論\n",
    "def predict(prompt, max_token=4095, temperature=0.0001, top_p=0.0001, title=''):\n",
    "    free_memory() ## メモリ解放\n",
    "    token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    n_token_input = len(token_ids[0])\n",
    "    LOG.info('**'+title+'************************************************')\n",
    "    LOG.info('**'+model_name)\n",
    "    LOG.info('**'+gpu_info().rstrip('\\n'))\n",
    "    LOG.info('**'+gpu_mem().rstrip('\\n'))\n",
    "    LOG.info('**'+show_memory())\n",
    "    LOG.info('**input:')\n",
    "    LOG.info(prompt)\n",
    "    LOG.info('**output:')  \n",
    "    torch.manual_seed(0) ## 乱数初期化\n",
    "    ##　推論開始\n",
    "    stime = time.perf_counter() ## 計測開始\n",
    "    output_ids = model.generate(\n",
    "        input_ids=token_ids.to(model.device),\n",
    "        max_new_tokens=max_token, ## 生成するトークンの最大数(プロンプトのトークンを含まない)\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        # top_k=0,\n",
    "        do_sample=True, ## Top-KやTop-p Samplingなどの戦略を有効にする\n",
    "        # num_beams=1 ## Multinomial Sampling 反復のリスクが軽減される（確率分布に基づいて次のトークンをランダムに選択）\n",
    "        # repetition_penalty=1 ## 繰り返しペナルティ、1.0はペナルティなし。モデルやユースケースに非常に敏感\n",
    "        # no_repeat_ngram_size=3, ## 単語の繰り返しがなくなるが 同じ単語が使えなくなるので長文生成などでは使いにくい\n",
    "        # pad_token_id=tokenizer.pad_token_id,\n",
    "        # bos_token_id=tokenizer.bos_token_id,\n",
    "        # eos_token_id=tokenizer.eos_token_id,\n",
    "        ## ArrowPro\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        ## llama3\n",
    "        # pad_token_id=tokenizer.eos_token_id,\n",
    "        # eos_token_id=[\n",
    "        #     tokenizer.eos_token_id,\n",
    "        #     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") ## llama3\n",
    "        # ],\n",
    "        streamer=streamer, ## ストリーミング表示する場合\n",
    "\n",
    "\n",
    "    )\n",
    "    tm = time.perf_counter() - stime ## 計測終了\n",
    "    n_token_output = len(output_ids[0][token_ids.size(1) :])\n",
    "\n",
    "    ## No Streaming\n",
    "    ## streamer=streamer をコメントすること\n",
    "    # output = tokenizer.decode(\n",
    "    #     output_ids[0][token_ids.size(1) :],\n",
    "    #     skip_special_tokens=True\n",
    "    # )\n",
    "    # LOG.info(output)\n",
    "    # End\n",
    "            \n",
    "    ## 計測結果\n",
    "    LOG.info('**Result: %s, Time: %.2f, Input: %d, Output: %d, Total: %d, Token/sec: %.1f' % (title, tm, n_token_input, n_token_output, n_token_input+n_token_output, n_token_output/tm)) ## 終了時間, 出力文字数    \n",
    "    LOG.info('**Result: %s, %s' % (title, gpu_mem().rstrip('\\n')))\n",
    "    LOG.info('**Result: %s, %s' % (title, show_memory()))\n",
    "    LOG.info('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "プロンプトテンプレート\n",
    "モデルに該当したプロンプトを指定する\n",
    "\"\"\"\n",
    "\n",
    "####\n",
    "## Vicuna1.5\n",
    "def qa_vicuna(input,\n",
    "      system='A chat between a human and an assistant.' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"{s}\n",
    "### Human: {i}\n",
    "### Assistant: \"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## llama2, elayza, stablelm\n",
    "def qa_llama2(input,\n",
    "      system='あなたは誠実で優秀な日本人のアシスタントです' ## システムプロンプト\n",
    "      ):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    return \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "        bos_token='<s>', ##tokenizer.bos_token,\n",
    "        b_inst=B_INST,\n",
    "        system=f\"{B_SYS}{system}{E_SYS}\",\n",
    "        prompt=input,\n",
    "        e_inst=E_INST,\n",
    "    )\n",
    "\n",
    "####\n",
    "## llama3\n",
    "def qa_llama3(input,\n",
    "      system='あなたは日本語を話すAIアシスタントです。日本語で回答してください。you MUST write Japanese language.' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"<|start_header_id|>system<|end_header_id|>\\n\\n{s}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{i}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## 東工大LLM/swallow\n",
    "def qa_swallow_chat(input,\n",
    "      system='以下に、あるタスクを説明する指示があります。リクエストを適切に完了するための回答を記述してください。' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"{s}\n",
    "    \n",
    "### 指示:\n",
    "{i}\n",
    "\n",
    "### 応答:\n",
    "\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## Gemma\n",
    "def qa_gemma(input,\n",
    "      system='' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"<start_of_turn>user\n",
    "{i}<end_of_turn>\n",
    "\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## ArrowPro\n",
    "## pad_token_id=tokenizer.eos_token_id\n",
    "def qa_arrowpro(user_query):\n",
    "    sys_msg = \"あなたは日本語を話す優秀なアシスタントです。回答には必ず日本語で答えてください。\"\n",
    "    template = \"\"\"[INST] <<SYS>>\n",
    "{}\n",
    "<</SYS>>\n",
    "\n",
    "{}[/INST]\"\"\"\n",
    "    return template.format(sys_msg,user_query)\n",
    "    \n",
    "\n",
    "## set Prompt Template\n",
    "qa = qa_arrowpro ## モデルに該当したプロンプトを指定する\n",
    "\n",
    "## for Test\n",
    "predict(qa(\"\"\"自己紹介をしてください。\"\"\"), title='自己紹介')\n",
    "\n",
    "\n",
    "LOG.info('fin.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
