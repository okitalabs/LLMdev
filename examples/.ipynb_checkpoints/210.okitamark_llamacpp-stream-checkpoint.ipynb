{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308f35b-4f73-4956-abe4-7bca9e989e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "okitamark\n",
    "llamacpp GGUF Stream\n",
    "\"\"\"\n",
    "\n",
    "## モデルとログファイルをセットする\n",
    "MODEL_NAME = '/home/users/model/DataPilot-ArrowPro-7B-KUJIRA-Q8_0.gguf' ## モデルの指定\n",
    "# LOG_FILE = __file__.replace('.py', '.log') ## デフォルトログファイル名, jupyterの時はNone\n",
    "LOG_FILE = None ## ファイル名 or None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Argument, Logging\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "## 引数の処理\n",
    "## jupyterの場合コメント\n",
    "# parser = argparse.ArgumentParser(description='OkitaMark')\n",
    "# parser.add_argument('--model', type=str, default=MODEL_NAME)\n",
    "# parser.add_argument('--log', type=str, default=LOG_FILE)\n",
    "# args = parser.parse_args()\n",
    "# MODEL_NAME = args.model\n",
    "# LOG_FILE = args.log\n",
    "\n",
    "## LOG_FILEが指定されていたら、ログファイルも出力する\n",
    "if LOG_FILE is not None:\n",
    "    handlers=[logging.FileHandler(LOG_FILE, mode='w'), logging.StreamHandler()]\n",
    "else:\n",
    "    handlers=[logging.StreamHandler()]\n",
    "\n",
    "## Logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s', handlers=handlers) ## Message Only\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LOG.info('MODEL_NAME: ' + MODEL_NAME)\n",
    "if LOG_FILE is not None: LOG.info('LOG_FILE:' + LOG_FILE)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Base\n",
    "\"\"\"\n",
    "import os, time\n",
    "import subprocess, argparse\n",
    "## Warning非表示\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "## Logging\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(message)s') ## Message Only\n",
    "# LOG = logging.getLogger(__name__)\n",
    "\n",
    "## Util ##\n",
    "## GPU Info\n",
    "def gpu_info():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=name --format=csv,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU device: ' + gpuinfo.decode()\n",
    "\n",
    "def gpu_mem():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used --format=csv,,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU memory: ' + gpuinfo.decode()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "モデルの定義\n",
    "llama-cpp\n",
    "\"\"\"\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_name = MODEL_NAME\n",
    "## init Model\n",
    "llm = Llama(\n",
    "    model_path=model_name,\n",
    "    n_ctx = 4095, ## Context Length\n",
    "    n_gpu_layers = -1, ## GPU Mem: -1=ALL, 0=None, N=n_layer\n",
    "    use_mlock = False, ## システムがモデルを RAM に保持するように強制する\n",
    "    chat_format = None,  ## create_chat_completion を呼び出すときに使用するチャット形式を指定する文字列\n",
    "    verbose = True, ## 詳細出力の出力\n",
    ")\n",
    "\n",
    "\n",
    "## 推論\n",
    "def predict(prompt, max_token=4095, temperature=0.0001, top_p=0.0001, seed=0, title=''):\n",
    "    LOG.info('**'+title+'************************************************')\n",
    "    LOG.info('**'+model_name)\n",
    "    LOG.info('**'+gpu_info().rstrip('\\n'))\n",
    "    LOG.info('**'+gpu_mem().rstrip('\\n'))\n",
    "    LOG.info('**input:')\n",
    "    LOG.info(prompt)\n",
    "    LOG.info('**output:')\n",
    "    \n",
    "    ##　推論開始\n",
    "    stime = time.perf_counter() ## 計測開始\n",
    "    output = llm.create_completion(\n",
    "        prompt,\n",
    "        max_tokens = max_token,\n",
    "        seed = seed,\n",
    "        temperature = temperature,\n",
    "        top_p = top_p,\n",
    "        # top_k = 100\n",
    "        # min_p = ,\n",
    "        # typical_p = ,\n",
    "        # frequency_penalty = 0,\n",
    "        # presence_penalty = 0,\n",
    "        # repeat_penalty = 1.1,\n",
    "        # tfs_z = 1,\n",
    "        # mirostat_mode =  0,\n",
    "        # mirostat_tau = 5,\n",
    "        # mirostat_eta = 0.1,\n",
    "        # stop=[\"Instruction:\", \"Input:\", \"Response:\", \"\\n\"],\n",
    "        # echo=True,\n",
    "        # stop=[\n",
    "        #     \"ASSISTANT:\",\n",
    "        #     \"USER:\",\n",
    "        #     \"SYSTEM:\",\n",
    "        #     \"### Human\",\n",
    "        #     \"### Assistant\",\n",
    "        # ],\n",
    "        stream=True,\n",
    "    )\n",
    "    tm = time.perf_counter() - stime ## 計測終了\n",
    "    \n",
    "    ## output\n",
    "    # out = output[\"choices\"][0][\"text\"]\n",
    "    # n_token_input = output['usage']['prompt_tokens']\n",
    "    # n_token_output = output['usage']['completion_tokens']\n",
    "    # n_token_total = output['usage']['total_tokens']\n",
    "\n",
    "    ## No Streaming\n",
    "    ## streamer=streamer をコメントすること\n",
    "    # LOG.info(out)\n",
    "    # End\n",
    "\n",
    "    ## Stream\n",
    "    for out in output:\n",
    "        s = out[\"choices\"][0][\"text\"]\n",
    "        print(s, end='')\n",
    "            \n",
    "    ## 計測結果\n",
    "    # LOG.info('**Result: %s, Time: %.2f, Input: %d, Output: %d, Total: %d, Token/sec: %.1f' % (title, tm, n_token_input, n_token_output, n_token_total, n_token_output/tm)) ## 終了時間, 出力文字数    \n",
    "    LOG.info('**Result: %s, Time: %.2f' %(title, tm))\n",
    "    LOG.info('**Result: %s, %s' % (title, gpu_mem().rstrip('\\n')))\n",
    "    LOG.info('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "プロンプトテンプレート\n",
    "モデルに該当したプロンプトを指定する\n",
    "\"\"\"\n",
    "\n",
    "####\n",
    "## Vicuna1.5\n",
    "def qa_vicuna(input,\n",
    "      system='A chat between a human and an assistant.' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"{s}\n",
    "### Human: {i}\n",
    "### Assistant: \"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## llama2, elayza, stablelm\n",
    "def qa_llama2(input,\n",
    "      system='あなたは誠実で優秀な日本人のアシスタントです' ## システムプロンプト\n",
    "      ):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    return \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "        bos_token='<s>', ##tokenizer.bos_token,\n",
    "        b_inst=B_INST,\n",
    "        system=f\"{B_SYS}{system}{E_SYS}\",\n",
    "        prompt=input,\n",
    "        e_inst=E_INST,\n",
    "    )\n",
    "\n",
    "####\n",
    "## llama3\n",
    "def qa_llama3(input,\n",
    "      system='あなたは日本語を話すAIアシスタントです。日本語で回答してください。you MUST write Japanese language.' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"<|start_header_id|>system<|end_header_id|>\\n\\n{s}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{i}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## 東工大LLM/swallow\n",
    "def qa_swallow_chat(input,\n",
    "      system='以下に、あるタスクを説明する指示があります。リクエストを適切に完了するための回答を記述してください。' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"{s}\n",
    "    \n",
    "### 指示:\n",
    "{i}\n",
    "\n",
    "### 応答:\n",
    "\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## Gemma\n",
    "def qa_gemma(input,\n",
    "      system='' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"<start_of_turn>user\n",
    "{i}<end_of_turn>\n",
    "\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## ArrowPro\n",
    "## pad_token_id=tokenizer.eos_token_id\n",
    "def qa_arrowpro(user_query):\n",
    "    sys_msg = \"あなたは日本語を話す優秀なアシスタントです。回答には必ず日本語で答えてください。\"\n",
    "    template = \"\"\"[INST] <<SYS>>\n",
    "{}\n",
    "<</SYS>>\n",
    "\n",
    "{}[/INST]\"\"\"\n",
    "    return template.format(sys_msg,user_query)\n",
    "    \n",
    "\n",
    "## set Prompt Template\n",
    "qa = qa_arrowpro ## モデルに該当したプロンプトを指定する\n",
    "\n",
    "## for Test\n",
    "predict(qa(\"\"\"自己紹介をしてください。\"\"\"), title='自己紹介')\n",
    "\n",
    "\n",
    "LOG.info('fin.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a90db2-fbc8-4974-8d82-0c582a03704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "計測メイン\n",
    "\"\"\"\n",
    "## 速度計測用、3回計測して中間の値を採用\n",
    "predict(qa('日本の魅力を10個教えてください。その詳細な理由述べてください。'), title='日本の魅力1')\n",
    "# predict(qa('日本の魅力を10個教えてください。その詳細な理由述べてください。'), title='日本の魅力2')\n",
    "# predict(qa('日本の魅力を10個教えてください。その詳細な理由述べてください。'), title='日本の魅力3')\n",
    "\n",
    "\n",
    "## 黙示録要約\n",
    "predict(qa(\"\"\"以下の文を1000文字程度に要約してください。\n",
    "\n",
    "小羊が第七の封印を解いた時、半時間ばかり天に静けさがあった。 2それからわたしは、神のみまえに立っている七人の御使を見た。そして、七つのラッパが彼らに与えられた。\n",
    "3また、別の御使が出てきて、金の香炉を手に持って祭壇の前に立った。たくさんの香が彼に与えられていたが、これは、すべての聖徒の祈に加えて、御座の前の金の祭壇の上にささげるためのものであった。 4香の煙は、御使の手から、聖徒たちの祈と共に神のみまえに立ちのぼった。 5御使はその香炉をとり、これに祭壇の火を満たして、地に投げつけた。すると、多くの雷鳴と、もろもろの声と、いなずまと、地震とが起った。\n",
    "6そこで、七つのラッパを持っている七人の御使が、それを吹く用意をした。\n",
    "7第一の御使が、ラッパを吹き鳴らした。すると、血のまじった雹と火とがあらわれて、地上に降ってきた。そして、地の三分の一が焼け、木の三分の一が焼け、また、すべての青草も焼けてしまった。\n",
    "8第二の御使が、ラッパを吹き鳴らした。すると、火の燃えさかっている大きな山のようなものが、海に投げ込まれた。そして、海の三分の一は血となり、 9海の中の造られた生き物の三分の一は死に、舟の三分の一がこわされてしまった。\n",
    "10第三の御使が、ラッパを吹き鳴らした。すると、たいまつのように燃えている大きな星が、空から落ちてきた。そしてそれは、川の三分の一とその水源との上に落ちた。 11この星の名は「苦よもぎ」と言い、水の三分の一が「苦よもぎ」のように苦くなった。水が苦くなったので、そのために多くの人が死んだ。\n",
    "12第四の御使が、ラッパを吹き鳴らした。すると、太陽の三分の一と、月の三分の一と、星の三分の一とが打たれて、これらのものの三分の一は暗くなり、昼の三分の一は明るくなくなり、夜も同じようになった。\n",
    "13また、わたしが見ていると、一羽のわしが中空を飛び、大きな声でこう言うのを聞いた、「ああ、わざわいだ、わざわいだ、地に住む人々は、わざわいだ。なお三人の御使がラッパを吹き鳴らそうとしている。\n",
    "1第五の御使が、ラッパを吹き鳴らした。するとわたしは、一つの星が天から地に落ちて来るのを見た。この星に、底知れぬ所の穴を開くかぎが与えられた。 2そして、この底知れぬ所の穴が開かれた。すると、その穴から煙が大きな炉の煙のように立ちのぼり、その穴の煙で、太陽も空気も暗くなった。 3その煙の中から、いなごが地上に出てきたが、地のさそりが持っているような力が、彼らに与えられた。 4彼らは、地の草やすべての青草、またすべての木をそこなってはならないが、額に神の印がない人たちには害を加えてもよいと、言い渡された。 5彼らは、人間を殺すことはしないで、五か月のあいだ苦しめることだけが許された。彼らの与える苦痛は、人がさそりにさされる時のような苦痛であった。 6その時には、人々は死を求めても与えられず、死にたいと願っても、死は逃げて行くのである。 7これらのいなごは、出陣の用意のととのえられた馬によく似ており、その頭には金の冠のようなものをつけ、その顔は人間の顔のようであり、 8また、そのかみの毛は女のかみのようであり、その歯はししの歯のようであった。 9また、鉄の胸当のような胸当をつけており、その羽の音は、馬に引かれて戦場に急ぐ多くの戦車の響きのようであった。 10その上、さそりのような尾と針とを持っている。その尾には、五か月のあいだ人間をそこなう力がある。 11彼らは、底知れぬ所の使を王にいただいており、その名をヘブル語でアバドンと言い、ギリシヤ語ではアポルオンと言う。\n",
    "12第一のわざわいは、過ぎ去った。見よ、この後、なお二つのわざわいが来る。\n",
    "13第六の御使が、ラッパを吹き鳴らした。すると、一つの声が、神のみまえにある金の祭壇の四つの角から出て、 14ラッパを持っている第六の御使にこう呼びかけるのを、わたしは聞いた。「大ユウフラテ川のほとりにつながれている四人の御使を、解いてやれ」。 15すると、その時、その日、その月、その年に備えておかれた四人の御使が、人間の三分の一を殺すために、解き放たれた。 16騎兵隊の数は二億であった。わたしはその数を聞いた。 17そして、まぼろしの中で、それらの馬とそれに乗っている者たちとを見ると、乗っている者たちは、火の色と青玉色と硫黄の色の胸当をつけていた。そして、それらの馬の頭はししの頭のようであって、その口から火と煙と硫黄とが、出ていた。 18この三つの災害、すなわち、彼らの口から出て来る火と煙と硫黄とによって、人間の三分の一は殺されてしまった。 19馬の力はその口と尾とにある。その尾はへびに似ていて、それに頭があり、その頭で人に害を加えるのである。 20これらの災害で殺されずに残った人々は、自分の手で造ったものについて、悔い改めようとせず、また悪霊のたぐいや、金・銀・銅・石・木で造られ、見ることも聞くことも歩くこともできない偶像を礼拝して、やめようともしなかった。 21また、彼らは、その犯した殺人や、まじないや、不品行や、盗みを悔い改めようとしなかった。\n",
    "\"\"\"), title='黙示録要約')\n",
    "\n",
    "\n",
    "## 日本の若者の将来\n",
    "predict(qa('日本の若者の将来について、できるだけ多く語ってください。'), title='日本の若者の将来')\n",
    "\n",
    "LOG.info('fin.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7abfc1-3dae-4bd8-a9bb-9867160802e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
