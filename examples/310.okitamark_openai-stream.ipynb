{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e378d-e1ba-49f5-bf15-37f82cc3214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "okitamark\n",
    "OpenAI API Stream\n",
    "\n",
    "LLMサーバを起動する\n",
    "```\n",
    "llamacpp-server  \\\n",
    "--chat-template vicuna \\\n",
    "--threads-batch 8 \\\n",
    "--threads-http 8 \\\n",
    "--model /home/users/model/vicuna-7b-v1.5.Q8_0.gguf \\\n",
    "--ctx-size 4095 \\\n",
    "--embeddings \\\n",
    "--parallel 8 \\\n",
    "--cont-batching \\\n",
    "--n-gpu-layers 33 \\\n",
    "--host 0.0.0.0 \\\n",
    "--port 8080\n",
    "```\n",
    "\"\"\"\n",
    "## モデルとログファイルをセットする\n",
    "MODEL_NAME = 'vicuna-7b-v1.5.Q8_0' ## API model名\n",
    "BASE_URL = \"http://localhost:8080/v1/\" ## OpenAI API Endpoint\n",
    "\n",
    "# LOG_FILE = __file__.replace('.py', '.log') ## デフォルトログファイル名, jupyterの時はNone\n",
    "LOG_FILE = None #'test.log' ## ファイル名 or None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Argument, Logging\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "## 引数の処理\n",
    "## jupyterの場合コメント\n",
    "# parser = argparse.ArgumentParser(description='OkitaMark')\n",
    "# parser.add_argument('--model', type=str, default=MODEL_NAME)\n",
    "# parser.add_argument('--log', type=str, default=LOG_FILE)\n",
    "# args = parser.parse_args()\n",
    "# MODEL_NAME = args.model\n",
    "# LOG_FILE = args.log\n",
    "\n",
    "## LOG_FILEが指定されていたら、ログファイルも出力する\n",
    "if LOG_FILE is not None:\n",
    "    handlers=[logging.FileHandler(LOG_FILE, mode='w'), logging.StreamHandler()]\n",
    "else:\n",
    "    handlers=[logging.StreamHandler()]\n",
    "\n",
    "## Logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s', handlers=handlers) ## Message Only\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LOG.info('MODEL_NAME: ' + MODEL_NAME)\n",
    "if LOG_FILE is not None: LOG.info('LOG_FILE:' + LOG_FILE)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Base\n",
    "\"\"\"\n",
    "import os, time\n",
    "import subprocess, argparse\n",
    "## Warning非表示\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "## Logging\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(message)s') ## Message Only\n",
    "# LOG = logging.getLogger(__name__)\n",
    "\n",
    "## Util ##\n",
    "## メモリ関連\n",
    "## GPU Info\n",
    "def gpu_info():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=name --format=csv,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU device: ' + gpuinfo.decode()\n",
    "\n",
    "def gpu_mem():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used --format=csv,,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU memory: ' + gpuinfo.decode()\n",
    "\n",
    "\n",
    "## 推論\n",
    "## LLMサーバ側でシステムプロンプトを指定していない場合、systemにシステムプロンプトを指定する\n",
    "import openai\n",
    "model_name = MODEL_NAME ## LLM Model Name: gpt-3.5-turbo, text-davinci-003\n",
    "\n",
    "def predict(prompt, max_token=4095, temperature=0.1, top_p=0.1, title='', system=''):\n",
    "    # token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # n_token_input = len(token_ids[0])\n",
    "    LOG.info('**'+title+'************************************************')\n",
    "    LOG.info('**'+model_name)\n",
    "    LOG.info('**'+gpu_info().rstrip('\\n'))\n",
    "    LOG.info('**'+gpu_mem().rstrip('\\n'))\n",
    "    LOG.info('**input:')\n",
    "    LOG.info(prompt)\n",
    "    ##　推論開始\n",
    "    stime = time.perf_counter() ## 計測開始\n",
    "    openai.api_key = \"EMPTY\" ## Dummy\n",
    "    openai.base_url = BASE_URL ## OpenAI API Endpoint\n",
    "\n",
    "    \"\"\" Chat completions \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "      model=model_name,\n",
    "      max_tokens=max_token,\n",
    "      temperature=temperature,\n",
    "      top_p=top_p, \n",
    "      seed=0,\n",
    "      stream=True, ## Streaming output\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    LOG.info('**output')\n",
    "    ## Streaming output\n",
    "    for chunk in response:\n",
    "        c = chunk.choices[0].delta.content\n",
    "        if c is not None:\n",
    "            print(c, end='')\n",
    "    tm = time.perf_counter() - stime ## 計測終了\n",
    "    # output = response.choices[0].message.content\n",
    "    # n_token_output = response.usage.completion_tokens\n",
    "    # n_token_input = response.usage.prompt_tokens\n",
    "    # n_token_total = response.usage.total_tokens\n",
    "\n",
    "            \n",
    "    ## 計測結果\n",
    "    # LOG.info(output) ## Streamingではコメントアウト\n",
    "    # LOG.info('**Result: %s, Time: %.2f, Input: %d, Output: %d, Total: %d, Token/sec: %.1f' % (title, tm, n_token_input, n_token_output, n_token_total, n_token_output/tm)) ## 終了時間, 出力文字数    \n",
    "    LOG.info('**Result: %s, %s' % (title, gpu_mem().rstrip('\\n')))\n",
    "    LOG.info('\\n\\n')\n",
    "\n",
    "\n",
    "## Prompt template\n",
    "def qa_openai(input):\n",
    "    return input\n",
    "qa = qa_openai\n",
    "\n",
    "\n",
    "predict(qa('自己紹介をしてください。'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
