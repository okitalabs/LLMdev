{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308f35b-4f73-4956-abe4-7bca9e989e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "okitamark\n",
    "llamacpp GGUF\n",
    "\"\"\"\n",
    "\n",
    "## モデルとログファイルをセットする\n",
    "MODEL_NAME = '/home/users/model/DataPilot-ArrowPro-7B-KUJIRA-Q8_0.gguf' ## モデルの指定\n",
    "# LOG_FILE = __file__.replace('.py', '.log') ## デフォルトログファイル名, jupyterの時はNone\n",
    "LOG_FILE = None ## ファイル名 or None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Argument, Logging\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "## 引数の処理\n",
    "## jupyterの場合コメント\n",
    "# parser = argparse.ArgumentParser(description='OkitaMark')\n",
    "# parser.add_argument('--model', type=str, default=MODEL_NAME)\n",
    "# parser.add_argument('--log', type=str, default=LOG_FILE)\n",
    "# args = parser.parse_args()\n",
    "# MODEL_NAME = args.model\n",
    "# LOG_FILE = args.log\n",
    "\n",
    "## LOG_FILEが指定されていたら、ログファイルも出力する\n",
    "if LOG_FILE is not None:\n",
    "    handlers=[logging.FileHandler(LOG_FILE, mode='w'), logging.StreamHandler()]\n",
    "else:\n",
    "    handlers=[logging.StreamHandler()]\n",
    "\n",
    "## Logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s', handlers=handlers) ## Message Only\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LOG.info('MODEL_NAME: ' + MODEL_NAME)\n",
    "if LOG_FILE is not None: LOG.info('LOG_FILE:' + LOG_FILE)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Base\n",
    "\"\"\"\n",
    "import os, time\n",
    "import subprocess, argparse\n",
    "## Warning非表示\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "## Logging\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(message)s') ## Message Only\n",
    "# LOG = logging.getLogger(__name__)\n",
    "\n",
    "## Util ##\n",
    "## GPU Info\n",
    "def gpu_info():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=name --format=csv,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU device: ' + gpuinfo.decode()\n",
    "\n",
    "def gpu_mem():\n",
    "    gpucmd = 'nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used --format=csv,,noheader'\n",
    "    gpuinfo = subprocess.check_output(gpucmd, shell=True)\n",
    "    return 'GPU memory: ' + gpuinfo.decode()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "モデルの定義\n",
    "llama-cpp\n",
    "\"\"\"\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_name = MODEL_NAME\n",
    "## init Model\n",
    "llm = Llama(\n",
    "    model_path=model_name,\n",
    "    n_ctx = 4095, ## Context Length\n",
    "    n_gpu_layers = -1, ## GPU Mem: -1=ALL, 0=None, N=n_layer\n",
    "    use_mlock = False, ## システムがモデルを RAM に保持するように強制する\n",
    "    chat_format = None,  ## create_chat_completion を呼び出すときに使用するチャット形式を指定する文字列\n",
    "    verbose = True, ## 詳細出力の出力\n",
    ")\n",
    "\n",
    "\n",
    "## 推論\n",
    "def predict(prompt, max_token=4095, temperature=0.0001, top_p=0.0001, seed=0, title=''):\n",
    "    LOG.info('**'+title+'************************************************')\n",
    "    LOG.info('**'+model_name)\n",
    "    LOG.info('**'+gpu_info().rstrip('\\n'))\n",
    "    LOG.info('**'+gpu_mem().rstrip('\\n'))\n",
    "    LOG.info('**input:')\n",
    "    LOG.info(prompt)\n",
    "    LOG.info('**output:')\n",
    "    \n",
    "    ##　推論開始\n",
    "    stime = time.perf_counter() ## 計測開始\n",
    "    output = llm.create_completion(\n",
    "        prompt,\n",
    "        max_tokens = max_token,\n",
    "        seed = seed,\n",
    "        temperature = temperature,\n",
    "        top_p = top_p,\n",
    "        # top_k = 100\n",
    "        # min_p = ,\n",
    "        # typical_p = ,\n",
    "        # frequency_penalty = 0,\n",
    "        # presence_penalty = 0,\n",
    "        # repeat_penalty = 1.1,\n",
    "        # tfs_z = 1,\n",
    "        # mirostat_mode =  0,\n",
    "        # mirostat_tau = 5,\n",
    "        # mirostat_eta = 0.1,\n",
    "        # stop=[\"Instruction:\", \"Input:\", \"Response:\", \"\\n\"],\n",
    "        # echo=True,\n",
    "        # stop=[\n",
    "        #     \"ASSISTANT:\",\n",
    "        #     \"USER:\",\n",
    "        #     \"SYSTEM:\",\n",
    "        #     \"### Human\",\n",
    "        #     \"### Assistant\",\n",
    "        # ],\n",
    "        # stream=True,\n",
    "    )\n",
    "    tm = time.perf_counter() - stime ## 計測終了\n",
    "    \n",
    "    ## output\n",
    "    out = output[\"choices\"][0][\"text\"]\n",
    "    n_token_input = output['usage']['prompt_tokens']\n",
    "    n_token_output = output['usage']['completion_tokens']\n",
    "    n_token_total = output['usage']['total_tokens']\n",
    "\n",
    "    ## No Streaming\n",
    "    ## streamer=streamer をコメントすること\n",
    "    LOG.info(out)\n",
    "    # End\n",
    "\n",
    "    # ## Stream\n",
    "    # for out in output:\n",
    "    #     s = out[\"choices\"][0][\"text\"]\n",
    "    #     print(s, end='')\n",
    "            \n",
    "    ## 計測結果\n",
    "    LOG.info('**Result: %s, Time: %.2f, Input: %d, Output: %d, Total: %d, Token/sec: %.1f' % (title, tm, n_token_input, n_token_output, n_token_total, n_token_output/tm)) ## 終了時間, 出力文字数    \n",
    "    LOG.info('**Result: %s, %s' % (title, gpu_mem().rstrip('\\n')))\n",
    "    LOG.info('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "プロンプトテンプレート\n",
    "モデルに該当したプロンプトを指定する\n",
    "\"\"\"\n",
    "\n",
    "####\n",
    "## Vicuna1.5\n",
    "def qa_vicuna(input,\n",
    "      system='A chat between a human and an assistant.' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"{s}\n",
    "### Human: {i}\n",
    "### Assistant: \"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## llama2, elayza, stablelm\n",
    "def qa_llama2(input,\n",
    "      system='あなたは誠実で優秀な日本人のアシスタントです' ## システムプロンプト\n",
    "      ):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    return \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "        bos_token='<s>', ##tokenizer.bos_token,\n",
    "        b_inst=B_INST,\n",
    "        system=f\"{B_SYS}{system}{E_SYS}\",\n",
    "        prompt=input,\n",
    "        e_inst=E_INST,\n",
    "    )\n",
    "\n",
    "####\n",
    "## llama3\n",
    "def qa_llama3(input,\n",
    "      system='あなたは日本語を話すAIアシスタントです。日本語で回答してください。you MUST write Japanese language.' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"<|start_header_id|>system<|end_header_id|>\\n\\n{s}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{i}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## 東工大LLM/swallow\n",
    "def qa_swallow_chat(input,\n",
    "      system='以下に、あるタスクを説明する指示があります。リクエストを適切に完了するための回答を記述してください。' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"{s}\n",
    "    \n",
    "### 指示:\n",
    "{i}\n",
    "\n",
    "### 応答:\n",
    "\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## Gemma\n",
    "def qa_gemma(input,\n",
    "      system='' ## システムプロンプト\n",
    "      ):\n",
    "    return \"\"\"<start_of_turn>user\n",
    "{i}<end_of_turn>\n",
    "\"\"\".format(s=system, i=input)\n",
    "\n",
    "####\n",
    "## ArrowPro\n",
    "## pad_token_id=tokenizer.eos_token_id\n",
    "def qa_arrowpro(user_query):\n",
    "    sys_msg = \"あなたは日本語を話す優秀なアシスタントです。回答には必ず日本語で答えてください。\"\n",
    "    template = \"\"\"[INST] <<SYS>>\n",
    "{}\n",
    "<</SYS>>\n",
    "\n",
    "{}[/INST]\"\"\"\n",
    "    return template.format(sys_msg,user_query)\n",
    "    \n",
    "\n",
    "## set Prompt Template\n",
    "qa = qa_arrowpro ## モデルに該当したプロンプトを指定する\n",
    "\n",
    "## for Test\n",
    "predict(qa(\"\"\"自己紹介をしてください。\"\"\"), title='自己紹介')\n",
    "\n",
    "\n",
    "LOG.info('fin.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
